ContrastiveTrain:
  _target_: chess_gnn.tasks.ContrastiveTrain
  model:
    _target_: chess_gnn.models.ChessContrastiveBackbone
    bert:
      _target_: chess_gnn.models.ChessBERT
      num_layers: 6
      block:
        _target_: chess_gnn.bert.TransformerBlock
        pos_emb_mode: "learned"
        hidden: 384
        attn_heads: 8
        feed_forward_hidden: 512
        norm_factory:
          _target_: chess_gnn.bert.utils.LayerNormFactory
      tokenizer:
        _target_: chess_gnn.tokenizers.SimpleChessTokenizer
      mask_handler:
        _target_: chess_gnn.bert.BERTMaskHandler
        mask_prob: 0.15
      optimizer_factory:
        _target_: chess_gnn.optimizers.LambFactory
        learning_rate: 1e-3
        weight_decay: 1e-4
      lr_scheduler_factory:
        _target_: chess_gnn.lr_schedules.CosineAnnealingWarmupFactory
        T_0: 10000
        T_mult: 2
        warmup_steps: 1000
      loss_weights:
        _target_: chess_gnn.models.BERTLossWeights
        masking: 1.0
        win_prediction: 1.0
    discriminator:
      _target_: chess_gnn.models.ChessDiscriminator
      num_layers: 12
      block:
        _target_: chess_gnn.bert.TransformerBlock
        pos_emb_mode: "learned"
        hidden: &dim
          512
        attn_heads: 8
        feed_forward_hidden: 512
        norm_factory:
          _target_: chess_gnn.bert.utils.LayerNormFactory
    student_mask_handler:
      _target_: chess_gnn.bert.ElectraMaskHandler
      mask_prob: 0.40
    teacher_mask_handler:
      _target_: chess_gnn.bert.ElectraMaskHandler
      mask_prob: 0.15
    clustering_head:
      _target_: chess_gnn.models.OnlineClustering
      in_dim: *dim
      out_dim: 8192
      n_sk_iter: 3
      target_temp: 0.06
      pred_temp: 0.12
    momentum: 0.99
    loss_weights:
      _target_: chess_gnn.models.ContrastiveLossWeights
      mlm: 1.0
      discriminator: 2.0
      contrastive: 0.25
      clustering: 1.0
    optimizer_factory:
      _target_: chess_gnn.optimizers.LambFactory
      learning_rate: 5e-5
      weight_decay: 1e-6
    lr_scheduler_factory:
      _target_: chess_gnn.lr_schedules.CosineAnnealingWarmupFactory
      T_0: 10000
      T_mult: 2
      warmup_steps: 1000
  datamodule:
    _target_: chess_gnn.data.ChessDataModule
    data_directory: '/home/ray/datasets/chess/lumbras_2024'
    batch_size: 1024
    num_workers: 8
    mode: 'ssl'
  trainer_factory:
    _target_: chess_gnn.trainer.TrainerFactory
    log_every_n_steps: 100
    max_epochs: 1
    accelerator: 'auto'
    strategy: 'ddp'
    devices: 1
    precision: "bf16-mixed"
    val_check_interval: 10_000
    num_sanity_val_steps: 1
    callbacks:
      - _target_: pytorch_lightning.callbacks.TQDMProgressBar
      - _target_: pytorch_lightning.callbacks.LearningRateMonitor
        logging_interval: 'step'
      - _target_: pytorch_lightning.callbacks.ModelCheckpoint
        _partial_: True
        save_last: True
        every_n_train_steps: 2500

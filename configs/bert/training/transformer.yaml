ChessTransformer:
  _target_: chess_gnn.models.ChessTransformer
  encoder:
    _target_: chess_gnn.models.ChessDiscriminator
    num_layers: 12
    block:
      _target_: chess_gnn.bert.TransformerBlock
      pos_emb_mode: "learned"
      hidden: 384
      attn_heads: 8
      feed_forward_hidden: 768
      norm_factory:
        _target_: chess_gnn.bert.utils.LayerNormFactory
  decoder:
    _target_: torch.nn.TransformerDecoder
    decoder_layer:
      _target_: torch.nn.TransformerDecoderLayer
      batch_first: True
      d_model: 256
      nhead: 8
      dim_feedforward: 1024
      dropout: 0.
    num_layers: 6
  mask_handler:
    _target_: chess_gnn.bert.TransformerMaskHandler
    masking_ratio: 0.25
  optimizer_factory:
    _target_: chess_gnn.optimizers.LambFactory
    learning_rate: 1e-3
    weight_decay: 1e-4
  lr_scheduler_factory:
    _target_: chess_gnn.lr_schedules.CosineAnnealingWarmupFactory
    T_0: 10000
    T_mult: 2
    warmup_steps: 1000

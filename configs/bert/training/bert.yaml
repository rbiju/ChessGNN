ChessBERT:
  _target_: chess_gnn.models.ChessBERT
  num_layers: 24
  block:
    _target_: chess_gnn.bert.TransformerBlock
    hidden: 256
    attn_heads: 16
    feed_forward_hidden: 512
    norm_factory:
      _target_: chess_gnn.bert.utils.DyTNormFactory
  tokenizer:
    _target_: chess_gnn.tokenizers.SimpleChessTokenizer
  mask_handler:
    _target_: chess_gnn.bert.BERTMaskHandler
  optimizer_factory:
    _target_: chess_gnn.optimizers.LambFactory
    learning_rate: 1e-3
    weight_decay: 1e-4
  lr_scheduler_factory:
    _target_: chess_gnn.lr_schedules.CosineAnnealingWarmupFactory
    T_max: 50
    warmup_epochs: 4
  loss_weights:
    _target_: chess_gnn.models.BERTLossWeights
    masking: 1.0
    win_prediction: 1.0
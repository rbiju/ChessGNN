ChessBERT:
  _target_: chess_gnn.models.ChessBERT
  num_layers: 12
  block:
    _target_: chess_gnn.bert.TransformerBlock
    pos_emb_mode: "relative"
    hidden: 256
    attn_heads: 8
    feed_forward_hidden: 1024
    norm_factory:
      _target_: chess_gnn.bert.utils.LayerNormFactory
  tokenizer:
    _target_: chess_gnn.tokenizers.SimpleChessTokenizer
  mask_handler:
    _target_: chess_gnn.bert.BERTMaskHandler
    mask_prob: 0.15
  optimizer_factory:
    _target_: chess_gnn.optimizers.LambFactory
    learning_rate: 1e-4
    weight_decay: 1e-4
  lr_scheduler_factory:
    _target_: chess_gnn.lr_schedules.CosineAnnealingWarmupFactory
    T_0: 10000
    warmup_steps: 1000
  loss_weights:
    _target_: chess_gnn.models.BERTLossWeights
    masking: 1.0
    win_prediction: 1.0